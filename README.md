# SysAdmCephCluster
###### Table of contents
1. [Architecture diagram](#diagram)
    1. [Architecture decisions and considerations](#arch_decesions)
    2. [Project components](#p_components)
    3. [Terraform structure](#tf)
    4. [Ansible structure](#ansible)
2. [Configurations](#configs)
3. [Backup statagies](#bk)
4. [System Troubleshooting](#trbl)
5. [System Recovery](#recovery)
6. [How to setup the ceph cluster project](#setup)

### Architecture diagram <a name="diagram"></a>
![image info](./resources/ceph_cluster_diagram.png)
#### Architecture decisions and considerations <a name="arch_decesions"></a>
#### Project Components <a name="p_components"></a>
#### Terraform structure <a name="tf"></a>
#### Ansible structure <a name="ansible"></a>
### Configurations <a name="configs"></a>
### Backup strategies <a name="bk"></a>
### Troubleshooting steps <a name="trbl"></a>
### System Recovery <a name="recovery"></a>

## How to setup the ceph project <a name="setup"></a>
### Prerequisites<a name="req"></a>
* google cloud cli installed
* ansible installed
* execute `ansible-galaxy collection install google.cloud` for ansible access gcp api
* terraform installed `version >=v1.6.1`
* authenticate with Google cloud cli `gcloud auth application-default login`
* Identity and Access Management (IAM) API enabled
* Compute Engine enabled (action made through GCP UI)
* install google auth library `pip install requests google-auth`
* install python requests library


### Enter into ansible folder
```bash
cd SysAdmCephCluster/ansible
```

#### Generate bastion ssh keys 
```bash
ansible-playbook -i inventory build_project/generate_ssh_keys.yaml --tags ssh_keys -vv
```
Example of a ansible command that generates the ssh keys that will be used to access all istances.

#### create tf state bucket, build tfvars and build inventory
```bash
ansible-playbook -i inventory build_project/main.yaml  --extra-vars "command=apply"  -vv
```
Example of a ansible command that generates the inventory used by ansible to retrieve all instances metadata deployed in the project. The data used from that API are: The private IP address, the public IP address from bastion host and the instance hostnames.
Furthermore, this playbook also generates the tf vars used by the terraform in this project.

The variables used in both ansible and terraform are set on `ceph_cluster_configuration.yml`  file in order to make the project as dynamic and adjustable to everyone.
Finally, is also created by this playbook the tf state bucket that will store all terraform states generated by each module that terraforms provision modules tfstates.
This ansible execution accepts as a command the option `apply` or `destroy`.


#### Execute terraform to setup the whole infrastructure infrastructure
##### Enter into terraform folder
```bash
cd SysAdmCephCluster/terraform
```
```bash
./executor.sh base apply
./executor.sh cephCluster apply
```

After the creation of the tfstate bucket, the following step has the goal of provisioning the whole infrastructure that will host the ceph cluster. The infrastructure in question is splitted into 2 modules the base and the cephCluster. Whereas the base module file is responsible for provision 1 VPC, 2 Subnets (private and public access subnet), firewall rules, Nat router and bastion host. On other hand, the cephCluster module is responsible for provisioning all resources directly related with the ceph cluster like the Manager and monitor nodes, the OSD, RDB, HDD volumes to be attached to each OSD VM.


##### Generate ssh proxy locally
```bash
ansible-playbook -i inventory bastion/build_local_proxy.yaml --tags apply --ask-become-pass  -vv
```

In order to access all the infrastructure, hosted in the private subnet (10.10.0.0/24), is crucial to create a proxy jump between the localhost where the ansible is being executed to the destination network. Therefore, regarding that the bastion host that is accessible from the public network on port 22 is being used as a proxy to jump to the Ceph Network.
The following image shows an example of a ssh_config file after executing the ansible command.
In this ansible command is passed the `proxy_jump` tag, and the `--ask-become-pass` to escalate priveligies to write into `/etc/ssh/ssh_config` file
<details open>
  <summary>/etc/ssh/ssh_config example</summary>
<IMG src=./resources/ssh_proxy_bastion.png></IMG>
</details>


##### Execute common configurations among all hosts
##### Command to apply in every node.
```bash
ansible-playbook -i inventory common/init.yaml -l all --tags ceph_init --key-file "../ssh_keys/idrsa"  -vv
```

​​After being granted connectivity to the entire environment it will begin the initial configurations with the assistance of ansible. Those configurations will grant the authentication between instances. Thus, it is necessary to copy the ssh keys generated previously in the beginning of the project to each instance to the bastion home directory.
In this ansible command is passed the parameters `-l all` to force the execution in every instance in the inventory, `--tags ceph_init` and the reference to the private ssh key used to authenticate in the instances `--key-file "../ssh_keys/idrsa"`.

##### Configure ceph monitor node
```bash
ansible-playbook -i inventory cephCluster/cephMonitor.yaml -l monitor --tags ceph_monitor,ceph_osd,ceph_rbd,ceph_manager,backup   --key-file "../ssh_keys/idrsa"  -vv
```
The cephMonitor playbook configures the monitor admin and the remaining monitors, and sends to the other ceph instances the ceph config file and the keyring to join the ceph cluster.

##### Configure ceph manager nodes
```bash
ansible-playbook -i inventory cephCluster/cephManager.yaml -l manager --tags ceph_manager --key-file "../ssh_keys/idrsa"  -vv
```
The cephManager playbook configures the manager nodes and enable the ceph dashboard

##### Configure ceph manager dashboard
```bash
ansible-playbook -i inventory cephCluster/cephManager.yaml -l manager --tags ceph_manager_dashboard --key-file "../ssh_keys/idrsa"  -vv
```
##### restore monitor
```bash
ansible-playbook -i inventory cephCluster/cephManager.yaml -l manager --tags ceph_monitor_restore --key-file "../ssh_keys/idrsa"  -vv
```
##### configure osd node
```bash
ansible-playbook -i inventory cephCluster/cephOSD.yaml -l osd --tags ceph_osd --key-file "../ssh_keys/idrsa"  -vv
```
The cephOSD playbook formats the entire HDD volume and mount it on `/dev/sdb1`, `/dev/sdc1`, ...
<details open>
  <summary>OSD Node status</summary>
<IMG src=./resources/osd.png></IMG>
</details>

##### Configure rbd node
```bash
ansible-playbook -i inventory cephCluster/cephRBD.yaml -l rbd --tags ceph_rbd,database,backup --key-file "../ssh_keys/idrsa"  -vv
```
<details open>
  <summary>Client (RDB Node)</summary>
<IMG src=./resources/rdb_nodes.png></IMG>
</details>

##### restore database 
```bash
ansible-playbook -i inventory cephCluster/cephRBD.yaml -l rbd --tags restore --key-file "../ssh_keys/idrsa"  -vv
```
The cephRBD playbook mainly creates the RBD Pool, maps the block device, format the volume with XFS, and mount the volume into `/dev/rbd0`.


### Usefull commands
##### Remote access bastion host
```bash
cd SysAdmCephCluster
ssh -i ssh_keys/idrsa  bastion@(bastion public ip)
```
##### Remote access ceph instances
```bash
cd SysAdmCephCluster/ansible
ssh (ceph instance private ip)
```

##### ceph commands to check the cluster status
```bash
ceph -s
ceph osd tree
ceph df 
ceph osd df 
```


##### Access dashboard via proxyjump
```bash
cd SysAdmCephCluster
ssh -i ../ssh_keys/idrsa -L 127.0.0.1:8443:(ceph_manager_private_address):8443 bastion@(bastion_public_address)
```


### Destroy cluster

```bash
cd SysAdmCephCluster/terraform

./executor.sh cephCluster destroy
./executor.sh base destroy
```

### Destroy tf state bucket

```bash
cd SysAdmCephCluster/ansible

ansible-playbook -i inventory build_project/main.yaml  --extra-vars "command=destroy"  -vv
```


#### restore monitor

ansible-playbook -i inventory cephCluster/cephManager.yaml -l manager --tags ceph_monitor_restore --key-file "../ssh_keys/idrsa"  -vv

### postgres commands
<details open>
  <summary>fist 50 results from dummy database</summary>
    
  ```bash root@rbd1:~# sudo -u postgres psql -c "select * from randomtable      limit  50;"
   id |           random_text            |   random_number    | random_integer | random_date
  ----+----------------------------------+--------------------+----------------+-------------
    1 | b06af2d3106b653d77a0625eb1cafb2a |  196.1698110577663 |             88 | 2567-10-01
    2 | 60e44f1937935bb73bd46b4c6ae5b1b8 | 187.50099342575544 |            507 | 2419-08-10
    3 | 05f0476cc36b58b233b9116a82100b78 |  642.6980756701013 |             23 | 2506-11-14
    4 | e0a0d554c343e7c7e1cf312ff780646d |  92.62785448726873 |            514 | 2432-04-26
    5 | 2e3beff40cb537281d1a1ce481509686 |  87.55502664444026 |            582 | 2578-02-14
    6 | 5f40d2945c8443dc7e64bbaad71db96c |  412.5930307965775 |            711 | 2236-07-17
    7 | d4f49b3c04dc6b8f0052aabb952d078a |  724.6586797408154 |            279 | 2766-07-23
    8 | 8c5325d8445178dc423270824c05a4bf | 165.49187002089784 |            229 | 2755-12-01
    9 | 6460f21ab04f7a3d58b40ea76ea4d125 |    779.99208007664 |            852 | 2163-12-10
   10 | 8192919c71cd29220c1c46169afb080d |  961.6014742822747 |            287 | 2723-01-12
   11 | 3b95a5bc1ff228bb85ba0db8200853ac | 488.08139204023627 |            317 | 2213-12-13
   12 | b6b81856cb1b627e919712b2387b0295 |  770.0333900128958 |            624 | 2330-02-08
   13 | 82ba1d7159d979225bac2dd197784086 | 360.72665659398575 |            936 | 2447-04-19
   14 | 230f5cdb59dba8bdddd393be52bd0338 |  7.678168694612797 |            424 | 2337-04-15
   15 | 60a9d75251a602d34abdcfa258d4e687 |  544.3859142302358 |            941 | 2005-05-12
   16 | 865768fd8df6a7a88681a1245d95b1e3 |  145.1951243013969 |            508 | 2174-01-18
   17 | f8fcd87d6bd84a431e6f4eac494ef6c2 |  265.7649323542377 |            741 | 2145-01-25
   18 | ba05450928345f17e083019f5f2c962b |  685.9257099105307 |            271 | 2550-02-19
  ```
</details>

<details open>
  <summary>crontab logs</summary>

  ```bash root@rbd1:~# tail -f /home/bastion/crontab.log
  number of backups =>0
  postgresql service is =>active
  number of backups =>1
  postgresql service is =>active
  number of backups =>2
  postgresql service is =>active
  number of backups =>3
  postgresql service is =>active
  number of backups =>4
  postgresql service is =>active
  number of backups =>5
  postgresql service is =>active
  number of backups exceeds maximum of 5
  removing oldest backup database_bk_1702043521.tar.gz
  generating backup => database_bk_1702043821.tar.gz
  number of backups =>5
  postgresql service is =>active
  number of backups exceeds maximum of 5
  removing oldest backup database_bk_1702043581.tar.gz
  generating backup => database_bk_1702043881.tar.gz
  number of backups =>5
  postgresql service is =>inactive
  number of backups =>5
  postgresql service is =>inactive
  ```
</details>
